{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the file. This means getting rid of duplicates; you can assume that no student can register for the same course more than once. How many duplicate records do you find? Some of the fields have bad or missing values; repair those that you can (and explain what a repair means)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import hashlib\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA_FILL_VALUE will replace missing values in the data with 0\n",
    "NA_FILL_VALUE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of rows in the data set prior to filtering out bad, missing, or corrupt lines\n",
    "# use the number to compare the size of the data set after filtering \n",
    "columnNames = []\n",
    "with open('dirty_sample_small_header.csv', 'r') as headerFile:\n",
    "    headerReader = csv.reader(headerFile, delimiter=',')\n",
    "    for row in headerReader:\n",
    "        columnNames.append(row[1])\n",
    "        \n",
    "numCols = len(columnNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon initial inspection, the \"ip\" column contains values do not match the format of an ip address. Each column after ip also has incompatible row entires. The \"viewed\" values must be missing under its appropriate column header because it is logically infeasible to have viewed, explored, or completed a course without registering for it, or for exploring a course without viewing it. As a result of the shift, the last column, the former cert_status values are eliminated. This is reasonable because the numbers previously under cert_status resembled times, and the values previously to the left column of cert_status were statuses ie: \"downloadable\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped: 15708   Duplicates: 595797   Missing: 6275   Kept: 43706   Total: 661486\n"
     ]
    }
   ],
   "source": [
    "invalidCols = 0; duplicateRows = 0; keptRows = 0; missingCols = 0; totalRows = 0\n",
    "onHeader = True\n",
    "hashes = set()\n",
    "with open('dirty_sample_small.csv', 'r') as dataFile:\n",
    "    with open('valid_rows_sample_small.csv', 'w') as outFile:\n",
    "        dataReader = csv.reader(dataFile, delimiter=',')\n",
    "        outWriter = csv.writer(outFile, delimiter = ',')\n",
    "        for row in dataReader:\n",
    "            # Skip the header line\n",
    "            if onHeader:\n",
    "                # Because the column \"viewed\" has data that are logically inconsistent, drop it.  \n",
    "                # Some entries have \"Registered=True\" and \"Viewed=False\". The values seem to actually be more similar\n",
    "                # to \"explored\" than viewed.  Additionally including one of viewed, explored, certified, or completed\n",
    "                # makes the column headers not align with the data so we need to drop one of them. \n",
    "                row = list(filter(lambda x: x != \"viewed\", row))\n",
    "                outWriter.writerow(row)\n",
    "                onHeader = False; continue\n",
    "        \n",
    "            totalRows += 1\n",
    "            # Ignore rows with incorrect number of columns\n",
    "            if len(row) != numCols:\n",
    "                invalidCols += 1\n",
    "                continue \n",
    "            # Get the md5 hash of each row to determine whether the row is duplicated or not\n",
    "            else:\n",
    "                m = hashlib.md5()\n",
    "                \n",
    "                # Two rows are identical if they have the same (course_id, student_id)\n",
    "                # fullRowStr = reduce((lambda x, y: x + y), row).encode('utf-8')\n",
    "                rowStr = (row[0] + row[1]).encode('utf-8')\n",
    "                m.update(rowStr)\n",
    "                hashedRow = m.hexdigest()\n",
    "                if hashedRow in hashes:\n",
    "                    duplicateRows += 1\n",
    "                    continue \n",
    "                # If it's a new row, write it to the cleaned dataset valid_rows...\n",
    "                else:\n",
    "                    hashes.add(hashedRow)\n",
    "                    # Ignore rows where more than 1/2 of the entries are missing\n",
    "                    # Count the number of nan's in a row\n",
    "                    missingFields = reduce(lambda x, y: x + int(y == \"\"), row, 0) # do not change \"\" to ''\n",
    "                    \n",
    "                    if missingFields >= int(0.5 * numCols):\n",
    "                        missingCols += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        keptRows += 1\n",
    "                        # Also ignore the data in the very last column as it does not seem to correspond to any of the \n",
    "                        # columns in this area of the dataset\n",
    "                        outWriter.writerow(row[:-1])\n",
    "print(\"Dropped: %d   Duplicates: %d   Missing: %d   Kept: %d   Total: %d\" % (invalidCols, duplicateRows, missingCols,\n",
    "                                                                             keptRows, totalRows))\n",
    "\n",
    "# If we only drop duplicates that match on all fields these are the results.   \n",
    "# Dropped: 15708   Duplicates: 595797   Kept: 49981   Total: 661486"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing lines that were corrupted (they had a number of entries that differed from the number specified by the header file), we found that there were 595797 duplicate rows. The remaining 49981 rows, while not perfect, seem to be have empty fields for roughly the same number of columns – if we keep rows that have a value in at least 55% of their fields then we keep all the de-duplicated rows, but if this theshold is raised to 60% we drop all of those – suggesting that some of these fields may not be very useful for data analysis. For most of these missing values, there is not much we can do. However, for the count variables (the one's like \"n &lt; noun &gt;\") we set missing values to be 0 because we think it is unlikely that students would post and EdX would not count that.  Additionally for birthdays, which are the only user inputed field in this dataset, if the student provides a birthday that would indicate the student is older than 100 or younger than 12 we set that birthday to be the average (mean) birthday. The mean was calculated excluding bad birthdays. This change would reduce the variance of any estimator that uses the birthday but roughly preserves the mean birthday (because the replacement YoB is rounded to the nearest whole year).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guarantees all rows are accounted for after filtering data\n",
    "invalidCols + duplicateRows + missingCols + keptRows == totalRows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to repair bad or missing values, we must understand which columns these values come from, which type all the data in that column should be represented with, and how values ought to be be represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colToType = {\n",
    "    \"registered\" : bool, \n",
    "    \"explored\" : bool,\n",
    "    \"certified\" : bool,\n",
    "    \"completed\" : bool,\n",
    "    \"latitude\" : float, \n",
    "    \"longitude\" : float, \n",
    "    \"YoB\" : int, \n",
    "    \"start_time\" : \"date\",\n",
    "    \"first_event\" : \"date\",\n",
    "    \"last_event\" : \"date\", \n",
    "    \"nevents\" : int, \n",
    "    \"ndays_act\" : int, \n",
    "    \"nplay_video\" : int,\n",
    "    \"nchapters\" : int, \n",
    "    \"nforum_posts\" : int, \n",
    "    \"nforum_votes\" : int, \n",
    "    \"nforum_endorsed\" : int, \n",
    "    \"nforum_threads\" : int, \n",
    "    \"nforum_comments\" : int, \n",
    "    \"nforum_pinned\" : int, \n",
    "    \"nprogcheck\" : int, \n",
    "    \"nproblem_check\" : int, \n",
    "    \"nforum_events\" : int, \n",
    "    # encoded as \"0\" or \"1\" (not \"False\" or \"True\"), need to convert to bool after converting to int\n",
    "    \"is_active\" : int, \n",
    "    \"cert_created_date\" : \"date\", \n",
    "    \"cert_modified_date\" : \"date\"    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToBool(x):\n",
    "    if x == 'True': return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicated Valid Rows: 43706\tFully Deduplicated: True\n",
      "Columns: 47\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"valid_rows_sample_small.csv\", sep=',', engine='python', error_bad_lines=False, dtype='unicode')\n",
    "\n",
    "# Use Pandas drop_duplicates() as evidence that dataset is deduplicated\n",
    "print(\"Deduplicated Valid Rows: %d\\tFully Deduplicated: %r\" \n",
    "      % (len(df_test), len(df_test) == len(df_test.drop_duplicates())))\n",
    "print(\"Columns: %d\" % len(df_test.columns.values))\n",
    "\n",
    "# Convert types of columns\n",
    "for colName, colType in colToType.items():\n",
    "    if colType == \"date\":\n",
    "        df_test[colName] = pd.to_datetime(df_test[colName])\n",
    "    elif colType == int:\n",
    "        df_test[colName] = df_test[colName].apply(lambda x: x if x != 'nan' else 0).astype(int)\n",
    "    elif colType == float:\n",
    "        df_test[colName] = df_test[colName].apply(lambda x: x if x != 'nan' else float('nan')).astype(float)\n",
    "    elif colType == bool:\n",
    "        df_test[colName] = df_test[colName].apply(lambda x: convertToBool(x))\n",
    "\n",
    "# special case for is_active\n",
    "df_test.is_active = df_test.is_active.astype(bool)\n",
    "\n",
    "# Reset extreme birthdays to mean birthday\n",
    "meanValidYoB = int(round(df_test[(df_test.YoB > 1920) & (df_test.YoB < 2008)].YoB.mean()))\n",
    "df_test.loc[:, 'YoB'] = df_test.YoB.apply(lambda x: meanValidYoB if x > 0 and (x <= 1920 or x >= 2008) else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some fields may have values that are incompatible types. This may occur when no data is stored for a variable, a user did not complete the course or course registration, or a column may contain multiple data types. A string representation of an age cannot be compared to a number. If a user inputted N/A, or left that field blank, it is interpreted differently as NA, na, NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.replace(\"nan\", np.nan, inplace=True)\n",
    "df_test.replace(\"None\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some lines may be corrupt; get rid of those or mark them in some way to show that they are not good lines. How many corrupt lines are there? Does the count of corrupt lines change if you get rid of them before getting rid of the duplicate records? What difference might this make to the remaining data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially the only difference between the code below and the code above is that instead of removing corrupt, incomplete lines before we remove duplicates, the first remove duplicates and then remove corrupt lines.  The count of corrupt lines did fall significantly (from 15708 to 498) once they're removed after the duplicates, but the total nnumber of records kept remained the same (49981). While in this specific dataset swapping these two procedures made produced identical cleaned datasets, in general these two operations may not commute, so to speak.  Because the lines are processed in the order in which they appear in the csv, if an incomplete row that also has the same (student_id, course_id) pair comes before a complete row witht the same id-paid, then in this case, we will completely eliminate this student from the dataset because the complete row(s) will be dropped by the deduplication and then the corrupt row will also be dropped afterwards.  Therefore, the order of filtering and processing in problem 1 is the better way because that will not allow a student such as this one to be removed.  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped: 3310   Duplicates: 506011   Missing: 110772   Kept: 41393   Total: 661486\n"
     ]
    }
   ],
   "source": [
    "invalidCols = 0; duplicateRows = 0; keptRows = 0; missingCols = 0; totalRows = 0\n",
    "onHeader = True\n",
    "hashes = set()\n",
    "with open('dirty_sample_small.csv', 'r') as dataFile:\n",
    "    with open('valid_rows_sample_small_2.csv', 'w') as outFile:\n",
    "        dataReader = csv.reader(dataFile, delimiter=',')\n",
    "        outWriter = csv.writer(outFile, delimiter = ',')\n",
    "        for row in dataReader:\n",
    "            # Skip the header line\n",
    "            if onHeader:\n",
    "                # Because the column \"viewed\" has data that are logically inconsistent, drop it.  \n",
    "                # Some entries have \"Registered=True\" and \"Viewed=False\". The values seem to actually be more similar\n",
    "                # to \"explored\" than viewed.  Additionally including one of viewed, explored, certified, or completed\n",
    "                # makes the column headers not align with the data so we need to drop one of them. \n",
    "                row = list(filter(lambda x: x != \"viewed\", row))\n",
    "                outWriter.writerow(row)\n",
    "                onHeader = False; continue\n",
    "        \n",
    "            totalRows += 1\n",
    "            \n",
    "            m = hashlib.md5()\n",
    "                \n",
    "            # Two rows are identical if they have the same (course_id, student_id)\n",
    "            # fullRowStr = reduce((lambda x, y: x + y), row).encode('utf-8')\n",
    "            rowStr = (row[0] + row[1]).encode('utf-8')\n",
    "            m.update(rowStr)\n",
    "            hashedRow = m.hexdigest()\n",
    "            if hashedRow in hashes:\n",
    "                duplicateRows += 1\n",
    "                continue \n",
    "            # If it's a new row, write it to the cleaned dataset valid_rows...\n",
    "            else:\n",
    "                # Ignore rows with incorrect number of columns\n",
    "                if len(row) != numCols:\n",
    "                    invalidCols += 1\n",
    "                    continue    \n",
    "                \n",
    "                # Ignore rows where more than 3/4 of the entries are missing\n",
    "                # Count the number of nan's in a row\n",
    "                missingFields = reduce(lambda x, y: x + int(y == \"\"), row, 0) # do not change \"\" to ''\n",
    "                if missingFields >= int(0.45 * numCols):\n",
    "                    missingCols += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    keptRows += 1\n",
    "                    hashes.add(hashedRow)\n",
    "                    # Also ignore the data in the very last column as it does not seem to correspond to any of the \n",
    "                    # columns in this area of the dataset\n",
    "                    outWriter.writerow(row[:-1])\n",
    "                        \n",
    "print(\"Dropped: %d   Duplicates: %d   Missing: %d   Kept: %d   Total: %d\" % (invalidCols, duplicateRows, missingCols,\n",
    "                                                                             keptRows, totalRows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some possible sources of bias in this data set? Is there anything unusual about the data set that you should flag?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the only user inputted data in this dataset were year of birth, gender, and level of education, generally, any data filled in by the user can introudce bias into the analysis because there is no way to verify that data and certain groups of people may not be willing to tell EdX their true characteristics, beliefs, or demographics.  Additionally, if people have more than one account at the same time, take the class multiple times, or if multiple use the same account, then our results could also be biased.  \n",
    "    \n",
    "It is unusual that when we load the original data in, the column headers to not seem to match the data entirely. For example, the \"completed\" field contains IP addresses, and more generally all of the data seems to have been shifted over by 1 column. This shifting was likely caused by losing the data that should have been in the \"viewed\" column.  Of all five boolean columns at the beginning (registered, viewed, explored, certified), the data in \"viewed\" are inconsistent with the other columns and more likely resemble what \"explored\" should contain.  Some entries have \"Registered=True\" and \"Viewed=False\". As a result, we removed \"viewed\" from our headers and also dropped the rightmost column in the dataset that was now without a header, but also doesn't seem to correspond to any other possible column. \n",
    "It was also unusual the last column of the data had data entries that did not seem to match the column header which was supposeduly indicating a certificate status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
