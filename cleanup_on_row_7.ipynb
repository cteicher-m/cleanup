{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the file. This means getting rid of duplicates; you can assume that no student can register for the same course more than once. How many duplicate records do you find? Some of the fields have bad or missing values; repair those that you can (and explain what a repair means)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import hashlib\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_FILL_VALUE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNames = []\n",
    "with open('dirty_sample_small_header.csv', 'r') as headerFile:\n",
    "    headerReader = csv.reader(headerFile, delimiter=',')\n",
    "    for row in headerReader:\n",
    "        columnNames.append(row[1])\n",
    "        \n",
    "numCols = len(columnNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "invalidCols = 0; duplicateRows = 0; keptRows = 0; totalRows = 0\n",
    "onHeader = True\n",
    "hashes = set()\n",
    "with open('dirty_sample_small.csv', 'r') as dataFile:\n",
    "    with open('valid_rows_sample_small.csv', 'w') as outFile:\n",
    "        dataReader = csv.reader(dataFile, delimiter=',')\n",
    "        outWriter = csv.writer(outFile, delimiter = ',')\n",
    "        for row in dataReader:\n",
    "            # Skip the header line\n",
    "            if onHeader:\n",
    "                # Because the column \"viewed\" has data that is logically inconsistent, drop it.  \n",
    "                # Some entries have \"Registered=True\" and \"Viewed=False\". The values seem to actually be more similar\n",
    "                # to \"explored\" than viewed.  Additionally including one of viewed, explored, certified, or completed\n",
    "                # makes the column headers not align with the data so we need to drop one of them. \n",
    "                row = list(filter(lambda x: x != \"viewed\", row))\n",
    "                outWriter.writerow(row)\n",
    "                onHeader = False; continue\n",
    "        \n",
    "            totalRows += 1\n",
    "            # Ignore rows with incorrect number of columns\n",
    "            if len(row) != numCols:\n",
    "                invalidCols += 1\n",
    "                continue \n",
    "            # Get the md5 hash of each row to determine whether the row is duplicated or not\n",
    "            else:\n",
    "                m = hashlib.md5()\n",
    "                \n",
    "                # Two rows are identical if they have the same (course_id, student_id)\n",
    "                # fullRowStr = reduce((lambda x, y: x + y), row).encode('utf-8')\n",
    "                rowStr = (row[0] + row[1]).encode('utf-8')\n",
    "                m.update(rowStr)\n",
    "                hashedRow = m.hexdigest()\n",
    "                if hashedRow in hashes:\n",
    "                    duplicateRows += 1\n",
    "                    continue \n",
    "                # If it's a new row, write it to the cleaned dataset valid_rows...\n",
    "                else:            \n",
    "                    keptRows += 1\n",
    "                    hashes.add(hashedRow)\n",
    "                    # Also ignore the data in the very last column as it does not seem to correspond to any of the \n",
    "                    # columns in this area of the dataset\n",
    "                    outWriter.writerow(row[:-1])\n",
    "print(\"Dropped: %d   Duplicates: %d   Kept: %d   Total: %d\" % (invalidCols, duplicateRows, keptRows, totalRows))\n",
    "\n",
    "# If we only drop duplicates that match on all fields these are the results.   \n",
    "# Dropped: 15708   Duplicates: 595797   Kept: 49981   Total: 661486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalidCols + duplicateRows + keptRows == totalRows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colToType = {\n",
    "    \"registered\" : bool, \n",
    "    \"explored\" : bool,\n",
    "    \"certified\" : bool,\n",
    "    \"completed\" : bool,\n",
    "    \"latitude\" : float, \n",
    "    \"longitude\" : float, \n",
    "    \"YoB\" : int, \n",
    "    \"start_time\" : \"date\",\n",
    "    \"first_event\" : \"date\",\n",
    "    \"last_event\" : \"date\", \n",
    "    \"nevents\" : int, \n",
    "    \"ndays_act\" : int, \n",
    "    \"nplay_video\" : int,\n",
    "    \"nchapters\" : int, \n",
    "    \"nforum_posts\" : int, \n",
    "    \"nforum_votes\" : int, \n",
    "    \"nforum_endorsed\" : int, \n",
    "    \"nforum_threads\" : int, \n",
    "    \"nforum_comments\" : int, \n",
    "    \"nforum_pinned\" : int, \n",
    "    \"nprogcheck\" : int, \n",
    "    \"nproblem_check\" : int, \n",
    "    \"nforum_events\" : int, \n",
    "    # encoded as \"0\" or \"1\" (not \"False\" or \"True\"), need to convert to bool after converting to int\n",
    "    \"is_active\" : int, \n",
    "    \"cert_created_date\" : \"date\", \n",
    "    \"cert_modified_date\" : \"date\"    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToBool(x):\n",
    "    if x == 'True': return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"valid_rows_sample_small.csv\", sep=',', engine='python', error_bad_lines=False, dtype='unicode')\n",
    "\n",
    "# Use Pandas drop_duplicates() as evidence that dataset is deduplicated\n",
    "print(\"Deduplicated Valid Rows: %d\\tFully Deduplicated: %r\" \n",
    "      % (len(df_test), len(df_test) == len(df_test.drop_duplicates())))\n",
    "print(\"Columns: %d\" % len(df_test.columns.values))\n",
    "\n",
    "# Convert types of columns\n",
    "for colName, colType in colToType.items():\n",
    "    if colType == \"date\":\n",
    "        df_test[colName] = pd.to_datetime(df_test[colName])\n",
    "    elif colType == int:\n",
    "        df_test[colName] = df_test[colName].apply(lambda x: x if x != 'nan' else 0).astype(int)\n",
    "    elif colType == float:\n",
    "        df_test[colName] = df_test[colName].apply(lambda x: x if x != 'nan' else float('nan')).astype(float)\n",
    "    elif colType == bool:\n",
    "        df_test[colName] = df_test[colName].apply(lambda x: convertToBool(x))\n",
    "\n",
    "# special case for is_active\n",
    "df_test.is_active = df_test.is_active.astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some fields may have values that are incompatible types. This may occur when no data is stored for a variable, a user did not complete the course or course registration, or a column may contain multiple data types. A string representation of an age cannot be compared to a number. If a user inputted N/A, or left that field blank, it is interpreted differently as NA, na, NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['LoE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_unique = df_test_no_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NA columns\n",
    "original_columns = set(df_test_unique.columns.values)\n",
    "df = df_test_unique.dropna(axis = 1, how = 'all').fillna(NA_FILL_VALUE)\n",
    "new_columns = set(df.columns.values)\n",
    "print(\"Removed columns\", original_columns - new_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to repair bad or missing values, we must understand which columns these values come from, which type all the data in that column should be represented with, and how empty values should be coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_repair = df_test_unique\n",
    "df_test_repair[df_test_repair.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some lines may be corrupt; get rid of those or mark them in some way to show that they are not good lines. How many corrupt lines are there? Does the count of corrupt lines change if you get rid of them before getting rid of the duplicate records? What difference might this make to the remaining data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line with no user id or course number is corrupt. The count of corrupt lines does not change if we get rid of them before or after the duplicate records. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some possible sources of bias in this data set? Is there anything unusual about the data set that you should flag?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students who registered more than once for the same class, or reigstered for the same class under different names, and students who enrolled but did not participate in a class to any extent, are possible sources of bias in this data set. The dataset has an enormous number of duplicate rows, and many unreliable birth dates. A student who did not pass the class due to failed assignments, versus a student who did not pass the class because he did not engage in the course beyond registration, are both represented as not passing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
