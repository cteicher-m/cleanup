{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the file. This means getting rid of duplicates; you can assume that no student can register for the same course more than once. How many duplicate records do you find? Some of the fields have bad or missing values; repair those that you can (and explain what a repair means)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import hashlib\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_FILL_VALUE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNames = []\n",
    "with open('dirty_sample_small_header.csv', 'r') as headerFile:\n",
    "    headerReader = csv.reader(headerFile, delimiter=',')\n",
    "    for row in headerReader:\n",
    "        columnNames.append(row[1])\n",
    "        \n",
    "numCols = len(columnNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "invalidCols = 0; duplicateRows = 0; keptRows = 0; totalRows = 0\n",
    "onHeader = True\n",
    "hashes = set()\n",
    "with open('dirty_sample_small.csv', 'r') as dataFile:\n",
    "    with open('valid_rows_sample_small.csv', 'w') as outFile:\n",
    "        dataReader = csv.reader(dataFile, delimiter=',')\n",
    "        outWriter = csv.writer(outFile, delimiter = ',')\n",
    "        for row in dataReader:\n",
    "            # Skip the header line\n",
    "            if onHeader:\n",
    "                outWriter.writerow(row)\n",
    "                onHeader = False; continue\n",
    "        \n",
    "            totalRows += 1\n",
    "            # Ignore rows with incorrect number of columns\n",
    "            if len(row) != numCols:\n",
    "                invalidCols += 1\n",
    "                continue \n",
    "            # Get the md5 hash of each row to determine whether the row is duplicated or not\n",
    "            else:\n",
    "                m = hashlib.md5()\n",
    "                rowStr = reduce((lambda x, y: x + y), row).encode('utf-8')\n",
    "                m.update(rowStr)\n",
    "                hashedRow = m.hexdigest()\n",
    "                if hashedRow in hashes:\n",
    "                    duplicateRows += 1\n",
    "                    continue \n",
    "                # If it's a new row, write it to the cleaned dataset valid_rows...\n",
    "                else:            \n",
    "                    keptRows += 1\n",
    "                    hashes.add(hashedRow)\n",
    "                    outWriter.writerow(row)\n",
    "            # Ignore rows where there are fewer entries than half the number of columns\n",
    "            count = 0\n",
    "            for word in row:\n",
    "                if word == \"nan\":\n",
    "                    count += 1\n",
    "            if count <= int(numCols/2):\n",
    "                continue \n",
    "print(\"Dropped: %d   Duplicates: %d   Kept: %d   Total: %d\" % (invalidCols, duplicateRows, keptRows, totalRows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalidCols + duplicateRows + keptRows == totalRows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"valid_rows_sample_small.csv\", sep=',', engine='python', error_bad_lines=False, dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas drop_duplicates() as evidence that dataset is deduplicated\n",
    "print(\"Deduplicated Valid Rows: %d\\tFully Deduplicated: %r\" \n",
    "      % (len(df_test), len(df_test) == len(df_test.drop_duplicates())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_test.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some fields may have values that are incompatible types. This may occur when no data is stored for a variable, a user did not complete the course or course registration, or a column may contain multiple data types. A string representation of an age cannot be compared to a number. If a user inputted N/A, or left that field blank, it is interpreted differently as NA, na, NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to repair bad or missing values, we must understand which columns these values come from, which type all the data in that column should be represented with, and how empty values should be coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.replace(\"nan\", np.nan, inplace=True)\n",
    "df_test.replace(\"None\", np.nan, inplace=True)\n",
    "nullCount = df_test.columns[df_test.isnull().any()]\n",
    "df_test[nullCount].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some lines may be corrupt; get rid of those or mark them in some way to show that they are not good lines. How many corrupt lines are there? Does the count of corrupt lines change if you get rid of them before getting rid of the duplicate records? What difference might this make to the remaining data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line with no user id or course number is corrupt. The count of corrupt lines does not change if we get rid of them before or after the duplicate records. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some possible sources of bias in this data set? Is there anything unusual about the data set that you should flag?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students who registered more than once for the same class, or reigstered for the same class under different names, and students who enrolled but did not participate in a class to any extent, are possible sources of bias in this data set. The dataset has an enormous number of duplicate rows, and many unreliable birth dates. Any self-reported data regarding a student's demographics will bias the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
